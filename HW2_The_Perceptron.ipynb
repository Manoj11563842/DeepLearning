{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYiZq0X2oB5t"
   },
   "source": [
    "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
    "\n",
    "# **HW1a The Perceptron** (20 pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGVmKzgG2Ium",
    "outputId": "4cc2ca21-861a-4fba-a38c-83e3ec04bec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
      "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
      "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
      "1\t0\t1\t1\t1\t0\t0\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t0\t1\t0\t1\t1\t1\t1\t1\t1\t1\t1\n",
      "0\t0\t1\t1\t0\t0\t1\t0\t0\t1\t1\t1\t1\t0\n",
      "1\t0\t1\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t1\n",
      "0\t0\t1\t1\t0\t0\t0\t0\t1\t0\t1\t1\t1\t0\n",
      "1\t0\t0\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\t0\n",
      "0\t1\t1\t1\t1\t1\t0\t0\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t0\t0\t0\t0\t1\t1\t0\t1\t1\t1\t1\t1\t0\n",
      "1\t0\t0\t1\t0\t1\t0\t1\t1\t0\t0\t1\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
      "1\t1\t1\t0\t0\t1\t1\t1\t1\t1\t0\t1\t1\t1\n",
      "0\t1\t1\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t0\t0\t1\t0\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
      "0\t0\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "1\t0\t1\t0\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t0\t0\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t0\t1\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t1\t0\t0\t0\t1\t1\t1\t1\t0\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t1\t1\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t1\t0\t0\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t1\t0\t1\t0\t1\t1\t0\t0\t0\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t0\t0\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t1\t0\n",
      "1\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "1\t1\t1\t1\t1\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "1\t1\t1\t0\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "1\t0\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "0\t1\t1\t0\t1\t0\t0\t0\t1\t0\t0\t0\t1\t0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 11244  100 11244    0     0   251k      0 --:--:-- --:--:-- --:--:--  255k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0\t1\t1\t1\t0\t0\t0\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t0\t1\t0\t1\t0\t0\t0\t0\t0\t0\t1\t1\t1\n",
      "1\t0\t0\t1\t0\t1\t0\t0\t0\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t1\t1\t1\t0\t1\t1\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t0\t0\t1\t0\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t1\n",
      "0\t1\t0\t0\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "0\t0\t1\t1\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t0\t0\t1\t0\t0\t1\t0\t1\t1\t0\n",
      "0\t0\t1\t0\t1\t1\t0\t0\t0\t1\t1\t0\t1\t0\n",
      "1\t0\t1\t0\t1\t0\t0\t0\t1\t0\t0\t1\t1\t1\n",
      "0\t0\t0\t1\t0\t1\t0\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t1\t0\t0\t1\t0\t1\t1\t0\t0\t1\t1\t1\t0\n",
      "0\t0\t1\t0\t1\t1\t1\t0\t0\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t0\t0\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "1\t0\t1\t0\t0\t0\t0\t1\t1\t1\t1\t0\t1\t1\n",
      "0\t1\t0\t1\t1\t1\t0\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\n",
      "0\t1\t0\t1\t0\t1\t1\t1\t0\t0\t0\t0\t1\t0\n",
      "1\t0\t1\t0\t1\t0\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t1\t0\t0\t1\t1\t1\t0\t0\t1\t1\t1\t1\t1\n",
      "1\t0\t1\t0\t1\t0\t1\t1\t0\t1\t1\t0\t1\t0\n",
      "1\t1\t0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t0\t1\t0\t0\t0\t1\t0\t0\t1\t0\n",
      "1\t0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\n",
      "1\t1\t0\t0\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "1\t0\t1\t0\t1\t1\t1\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t0\t1\t1\t0\t1\t1\t1\t0\t0\t1\t0\n",
      "1\t0\t0\t1\t1\t1\t0\t1\t0\t0\t0\t0\t1\t0\n",
      "1\t0\t1\t1\t1\t0\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t0\t0\t0\t1\t1\t1\t1\t0\n",
      "0\t1\t1\t0\t0\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "1\t1\t0\t0\t1\t0\t1\t0\t0\t1\t0\t0\t1\t0\n",
      "1\t0\t0\t0\t1\t1\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t1\t0\t0\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t1\t0\t1\t0\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t1\t1\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
      "1\t1\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t1\t0\t1\t0\t1\t0\t1\t1\t1\t1\t1\n",
      "0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t1\t1\t1\t0\t0\t0\t1\t1\n",
      "1\t0\t0\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t1\t0\t1\t0\t0\t1\t1\t0\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t0\t0\t0\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "0\t0\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\t1\t0\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t1\t1\t0\t0\t0\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t1\t0\t0\t1\t0\t0\t1\t1\t1\t0\t0\t1\t0\n",
      "1\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\t1\t1\n",
      "0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n",
      "1\t1\t1\t1\t1\t0\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t0\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t1\t1\t0\t1\t0\t0\t1\t0\n",
      "1\t0\t1\t0\t1\t0\t0\t1\t1\t0\t0\t1\t1\t0\n",
      "1\t0\t1\t0\t1\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t0\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t0\t0\t1\t1\t1\t1\t1\t0\t1\t0\t0\t1\t0\n",
      "1\t1\t1\t0\t0\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t1\t0\t0\t0\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t1\t1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t0\t1\t1\t1\n",
      "0\t1\t0\t0\t1\t0\t0\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t0\t1\t1\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t1\t1\t1\t1\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t0\t1\t0\t1\t1\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t1\t1\t0\t1\t1\t0\t0\t1\t1\t1\t0\n",
      "0\t0\t1\t0\t1\t1\t0\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\n",
      "1\t1\t1\t1\t0\t0\t0\t1\t0\t0\t1\t1\t1\t0\n",
      "0\t1\t1\t1\t1\t0\t1\t0\t1\t1\t1\t1\t1\t1\n",
      "0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t0\t1\t1\t0\t1\t1\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t1\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "0\t1\t0\t0\t1\t1\t1\t0\t1\t0\t1\t0\t1\t1\n",
      "0\t0\t1\t1\t1\t1\t0\t0\t0\t1\t1\t1\t1\t0\n",
      "1\t1\t0\t1\t0\t1\t1\t0\t0\t0\t0\t1\t1\t1\n",
      "1\t1\t1\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\n",
      "0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t1\t1\t1\t1\n",
      "1\t0\t1\t1\t1\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t0\t0\t1\t0\t0\t0\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t1\t0\t0\t1\t1\t1\t1\t0\t0\t1\t1\n",
      "1\t0\t0\t1\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t0\t1\t1\t0\t0\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t1\t1\t1\t1\t0\t1\t1\t1\t1\t1\t1\n",
      "0\t1\t0\t1\t0\t0\t0\t0\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
      "0\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\t1\t1\t0\n",
      "1\t1\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t0\t1\t0\t1\t1\t1\t0\t1\t1\t1\n",
      "1\t1\t1\t0\t0\t1\t1\t1\t0\t0\t0\t1\t1\t1\n",
      "0\t0\t0\t0\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t0\t1\t0\n",
      "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t1\t1\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t1\t1\t1\t1\t0\t1\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "1\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t1\n",
      "0\t0\t0\t0\t0\t0\t0\t0\t1\t1\t0\t1\t1\t0\n",
      "1\t1\t0\t0\t0\t1\t0\t0\t1\t1\t1\t1\t1\t1\n",
      "1\t0\t1\t0\t0\t0\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t0\t1\t0\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t0\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t0\t1\t0\t1\t0\t0\t1\t0\t1\t1\t0\n",
      "0\t0\t1\t0\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\n",
      "0\t0\t0\t0\t0\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t1\t1\t1\t0\t1\t1\t0\t1\t0\n",
      "1\t1\t0\t0\t0\t0\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t0\t1\t0\t1\t0\t0\t1\t0\t1\t1\t1\t1\t1\n",
      "1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t0\t1\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t1\t0\t0\t0\t0\t0\t0\t0\t1\t0\n",
      "1\t0\t0\t1\t0\t1\t0\t0\t0\t1\t1\t1\t1\t0\n",
      "1\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "0\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\t1\t1\t0\n",
      "0\t0\t0\t1\t0\t0\t0\t0\t1\t0\t1\t0\t1\t1\n",
      "1\t1\t0\t1\t1\t0\t1\t0\t1\t1\t0\t0\t1\t0\n",
      "1\t1\t1\t1\t1\t1\t0\t0\t0\t1\t1\t0\t1\t1\n",
      "1\t0\t1\t1\t1\t0\t0\t1\t0\t0\t0\t1\t1\t1\n",
      "1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t0\t0\t1\t1\t0\t1\t1\t1\t1\t1\t0\n",
      "0\t0\t1\t0\t1\t0\t0\t1\t0\t0\t1\t1\t1\t1\n",
      "1\t1\t0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t1\t1\t0\t1\t0\t1\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t0\t0\t1\t0\t0\t1\t0\t0\t1\t1\t0\n",
      "0\t0\t1\t1\t1\t0\t0\t1\t1\t1\t0\t1\t1\t1\n",
      "1\t1\t1\t1\t0\t1\t0\t0\t0\t0\t0\t1\t1\t0\n",
      "1\t1\t1\t0\t1\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t1\t1\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t0\t0\t0\t1\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\t1\t1\t1\n",
      "1\t1\t1\t0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t1\t1\t1\t1\t1\t1\t0\t1\t1\t0\t0\t1\t0\n",
      "1\t0\t0\t1\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t0\t1\t1\t1\t0\t1\t1\t0\t0\t0\t0\t1\t0\n",
      "1\t1\t0\t0\t1\t1\t1\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t0\t1\t0\t1\t1\t1\t0\t0\t1\t0\n",
      "1\t0\t1\t1\t0\t0\t0\t0\t1\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t1\t1\t1\t0\t1\t0\t0\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t1\n",
      "0\t0\t0\t1\t0\t1\t0\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\t1\t1\n",
      "0\t1\t0\t0\t1\t0\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t0\t1\t0\t0\t0\t1\t1\t0\t1\t1\t0\n",
      "0\t0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t1\t0\t0\t1\t0\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t0\t1\t1\t0\t0\t1\t1\t1\t1\t0\n",
      "0\t1\t0\t1\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t0\t1\t0\t0\t0\t0\t1\t0\t1\t0\n",
      "1\t1\t1\t0\t1\t1\t0\t0\t1\t0\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t0\t0\t0\t1\t1\t1\t0\t1\t1\t1\n",
      "0\t1\t1\t0\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t0\t0\t0\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t0\t0\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t1\t1\t1\t0\t0\t0\t1\t0\t0\t1\t0\n",
      "1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t0\t1\t1\t0\n",
      "1\t0\t1\t1\t1\t0\t0\t0\t0\t0\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t1\t0\t1\t0\t1\t0\t1\t1\n",
      "1\t1\t0\t1\t1\t1\t1\t0\t0\t1\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t0\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t1\t1\t0\t0\t1\t1\t1\t0\t1\t1\t1\n",
      "0\t0\t1\t0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\n",
      "0\t0\t1\t1\t1\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "0\t0\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t1\t0\t1\t1\t0\t0\t0\t1\t0\t1\t0\n",
      "1\t0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "1\t1\t1\t0\t0\t1\t0\t1\t1\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t0\t0\t1\t1\t0\t1\t0\t1\t0\t1\t1\n",
      "0\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\t0\t1\t0\n",
      "1\t1\t0\t0\t1\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t0\t1\t0\t0\t1\t0\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\n",
      "1\t1\t0\t1\t0\t0\t0\t0\t0\t1\t0\t0\t1\t1\n",
      "1\t0\t0\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\t0\n",
      "0\t1\t1\t1\t1\t0\t0\t1\t0\t0\t1\t0\t1\t0\n",
      "1\t0\t0\t1\t1\t1\t0\t0\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t0\t0\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
      "0\t1\t1\t0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "0\t1\t1\t0\t0\t1\t0\t0\t1\t0\t0\t1\t1\t0\n",
      "0\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t1\t0\n",
      "1\t0\t0\t0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\n",
      "1\t0\t1\t1\t0\t1\t0\t1\t0\t0\t1\t1\t1\t1\n",
      "0\t1\t1\t0\t0\t0\t0\t1\t0\t0\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\t0\n",
      "0\t0\t0\t1\t0\t0\t0\t0\t1\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t0\t1\t1\t1\t0\t1\t0\t1\t0\t1\t1\n",
      "0\t1\t0\t1\t0\t1\t0\t0\t0\t1\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t0\t0\t1\t0\t1\t0\t0\t0\t1\t0\n",
      "1\t1\t1\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t1\t1\t1\t0\t1\t1\t0\t1\t0\n",
      "1\t1\t0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t1\t0\t0\t1\t1\t1\t0\t1\t1\t0\t1\t1\t0\n",
      "0\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\t0\t1\t0\n",
      "1\t0\t1\t1\t0\t0\t1\t0\t1\t0\t1\t0\t1\t1\n",
      "0\t0\t0\t1\t1\t0\t1\t1\t0\t1\t1\t1\t1\t1\n",
      "1\t0\t1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\n",
      "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t0\t1\t1\n",
      "1\t0\t0\t0\t1\t1\t1\t1\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0\t0\t1\t1\n",
      "0\t1\t0\t1\t1\t0\t1\t1\t1\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t1\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t1\t1\t1\t0\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t0\t1\t1\t1\t0\t1\t0\t0\t1\t1\t1\n",
      "1\t1\t0\t0\t1\t0\t0\t1\t0\t0\t0\t0\t1\t0\n",
      "1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t1\n",
      "0\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t1\t0\t0\t1\t0\t0\t1\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\n",
      "1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "1\t1\t1\t1\t0\t0\t0\t1\t1\t1\t0\t0\t1\t1\n",
      "0\t1\t1\t1\t0\t0\t0\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t0\t1\t0\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
      "0\t1\t1\t1\t0\t1\t0\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t0\t1\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "0\t0\t1\t1\t0\t0\t0\t0\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t1\t1\t1\t0\t0\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t1\t1\t1\t1\t0\t0\t0\t0\t1\t0\n",
      "0\t0\t1\t1\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t0\t0\t0\t1\t0\t1\t1\t0\t0\t1\t0\n",
      "1\t1\t1\t0\t1\t0\t1\t0\t0\t1\t0\t0\t1\t0\n",
      "1\t1\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t0\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t1\t0\t1\t0\t1\t1\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t0\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t0\t1\t1\t0\t0\t0\t1\t0\t1\t0\n",
      "1\t0\t1\t0\t0\t1\t1\t1\t1\t1\t0\t0\t1\t1\n",
      "0\t1\t1\t1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t0\t1\t0\t1\t0\t1\t0\t1\t1\t0\t0\t1\t0\n",
      "1\t0\t1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\n",
      "1\t0\t0\t1\t0\t1\t1\t1\t1\t1\t0\t1\t1\t1\n",
      "1\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t1\t0\n",
      "0\t0\t1\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "1\t0\t1\t1\t0\t0\t1\t0\t0\t1\t1\t1\t1\t0\n",
      "0\t0\t1\t1\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "1\t1\t1\t1\t1\t0\t0\t1\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t1\t0\t0\t0\t1\t0\t1\t0\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t1\t1\t1\t1\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t1\t0\t1\t0\t0\t1\t1\t0\n",
      "0\t1\t1\t0\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\n",
      "1\t1\t0\t0\t1\t0\t1\t0\t0\t1\t0\t0\t1\t0\n",
      "1\t0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t0\t1\t0\t0\t1\t1\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t1\t0\t1\t1\t0\t1\t1\t0\t1\t0\n",
      "1\t0\t1\t1\t1\t0\t0\t0\t0\t1\t1\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t1\t0\t1\t0\t0\t1\t0\t1\t0\n",
      "1\t0\t1\t1\t1\t0\t1\t0\t1\t1\t0\t1\t1\t1\n",
      "1\t1\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\t0\n",
      "0\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t1\t1\t0\t0\t1\t0\t0\t0\t1\t0\t1\t0\n",
      "1\t1\t0\t1\t0\t1\t1\t0\t1\t1\t0\t1\t1\t0\n",
      "1\t0\t1\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "1\t1\t0\t0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t1\t0\t0\t0\t1\t1\t0\t1\t1\t0\t1\t0\n",
      "1\t0\t1\t0\t0\t0\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t0\t0\t1\t0\t0\t1\t0\t1\t0\n",
      "1\t0\t1\t0\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t1\t0\t1\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\n",
      "0\t1\t1\t0\t0\t1\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t0\t1\t0\t1\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
      "1\t1\t0\t1\t0\t0\t0\t0\t1\t0\t1\t1\t1\t0\n",
      "0\t0\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t1\t0\t1\t0\t0\t0\t1\t0\n",
      "1\t1\t0\t0\t1\t1\t1\t1\t0\t1\t0\t1\t1\t0\n",
      "1\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\n",
      "0\t1\t0\t1\t0\t0\t1\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "1\t0\t0\t0\t0\t0\t0\t0\t0\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t0\t0\t1\t0\n",
      "1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t1\t0\t0\t0\t0\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t0\t0\t1\t0\t0\t1\t0\t0\t0\t1\t1\t1\t0\n",
      "1\t0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "1\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "0\t1\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t0\t0\t0\t1\t0\t0\t1\t1\t1\t0\n",
      "0\t0\t1\t0\t1\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "1\t1\t0\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\t0\n",
      "1\t0\t1\t1\t1\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t0\t0\t1\t1\t0\t0\t1\t0\t1\t1\t1\n",
      "1\t0\t0\t1\t0\t0\t0\t1\t1\t1\t1\t0\t1\t1\n",
      "1\t1\t0\t1\t1\t0\t0\t0\t0\t1\t1\t0\t1\t1\n",
      "1\t0\t0\t0\t1\t0\t0\t0\t1\t0\t0\t1\t1\t0\n",
      "1\t1\t0\t1\t0\t1\t0\t0\t1\t1\t1\t1\t1\t1\n",
      "1\t0\t1\t0\t1\t1\t0\t0\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t0\t1\t0\t0\t1\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t1\t0\t0\t1\t0\t1\t0\t0\t0\t1\t1\t0\n",
      "0\t0\t1\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\n",
      "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t0\t1\t1\t1\t1\t0\t1\t1\n",
      "1\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\t1\t1\t1\n",
      "0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t1\t0\t1\t1\n",
      "1\t0\t0\t0\t1\t1\t0\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t0\t0\t1\t1\t1\t1\t1\t1\n",
      "1\t0\t1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t1\t1\t1\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t1\t1\t1\t0\t0\t0\t1\t0\t0\t1\t0\n",
      "1\t0\t1\t0\t0\t1\t0\t1\t1\t0\t1\t1\t1\t0\n",
      "0\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t1\t1\t0\t1\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t0\t1\t0\t1\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t0\t1\t0\t0\t1\t1\t0\t1\t0\n",
      "1\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\t1\t0\n",
      "0\t1\t0\t1\t0\t0\t0\t0\t1\t1\t0\t1\t1\t0\n",
      "1\t1\t0\t0\t0\t0\t1\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\t0\t1\t0\n",
      "1\t1\t0\t1\t0\t0\t0\t0\t0\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\t1\n",
      "0\t0\t0\t1\t0\t1\t1\t1\t0\t0\t1\t0\t1\t0\n",
      "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\n",
      "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  2844  100  2844    0     0  47538      0 --:--:-- --:--:-- --:--:-- 48203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n",
      "1\t0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "1\t0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
      "0\t0\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t1\t1\t1\t0\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t0\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t1\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "1\t0\t0\t1\t0\t0\t1\t1\t0\t0\t1\t1\t1\t0\n",
      "0\t0\t0\t1\t0\t1\t1\t0\t1\t1\t1\t1\t1\t0\n",
      "1\t0\t1\t0\t1\t1\t0\t0\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t0\n",
      "1\t1\t0\t1\t0\t1\t1\t0\t1\t0\t0\t0\t1\t0\n",
      "1\t0\t1\t1\t0\t1\t0\t0\t0\t0\t1\t1\t1\t1\n",
      "1\t0\t1\t1\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\n",
      "1\t1\t0\t1\t1\t0\t1\t1\t0\t0\t1\t0\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\t1\t0\n",
      "1\t0\t1\t0\t0\t1\t0\t0\t0\t0\t1\t1\t1\t0\n",
      "1\t0\t1\t1\t1\t1\t0\t0\t0\t0\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t1\t1\t0\t1\t1\t0\t1\t1\t1\n",
      "0\t1\t1\t1\t0\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "1\t0\t0\t0\t0\t0\t0\t1\t1\t1\t1\t0\t1\t1\n",
      "1\t0\t1\t1\t0\t1\t1\t0\t0\t1\t0\t1\t1\t0\n",
      "1\t1\t1\t0\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "0\t1\t0\t1\t0\t1\t0\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t0\t0\t0\t1\t1\t0\t1\t1\n",
      "1\t0\t1\t1\t0\t1\t1\t1\t0\t0\t1\t0\t1\t0\n",
      "1\t1\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t1\t1\n",
      "1\t1\t0\t1\t0\t0\t1\t0\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\t1\t1\t1\n",
      "1\t1\t1\t1\t1\t1\t1\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t0\t0\t0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t1\n",
      "1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t1\t0\t0\t1\t1\t0\t1\t0\t1\t1\t0\n",
      "0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
      "1\t0\t0\t1\t0\t0\t1\t1\t1\t1\t1\t1\t1\t0\n",
      "0\t1\t1\t1\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t1\t1\t1\t0\t1\t1\t0\t0\t0\t1\t0\t1\t0\n",
      "0\t0\t0\t0\t0\t0\t1\t0\t0\t1\t0\t1\t1\t1\n",
      "0\t1\t1\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\n",
      "0\t0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "0\t1\t1\t1\t1\t0\t1\t1\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t0\t0\t0\t1\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t1\t0\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "0\t1\t1\t0\t1\t1\t1\t0\t1\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t1\t1\t1\t0\t1\t0\t0\t1\t1\t0\n",
      "0\t0\t1\t0\t0\t0\t1\t1\t1\t1\t0\t0\t1\t1\n",
      "1\t1\t1\t1\t1\t0\t0\t0\t1\t1\t1\t0\t1\t0\n",
      "1\t0\t1\t1\t0\t1\t1\t0\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
      "1\t1\t1\t0\t0\t1\t1\t1\t0\t0\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t1\t0\t1\t0\t0\t1\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t0\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
      "1\t0\t0\t0\t1\t1\t1\t1\t0\t1\t0\t0\t1\t0\n",
      "1\t1\t1\t0\t0\t0\t1\t1\t1\t1\t0\t1\t1\t0\n",
      "0\t1\t1\t0\t0\t0\t1\t1\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t0\t1\t0\t0\t0\t0\t1\t1\t1\n",
      "1\t0\t0\t1\t0\t1\t1\t1\t1\t0\t1\t0\t1\t1\n",
      "1\t1\t0\t0\t0\t0\t0\t1\t0\t1\t1\t1\t1\t1\n",
      "1\t1\t1\t0\t0\t1\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t0\t1\t0\t1\t0\t0\t0\t1\t1\t1\t0\n",
      "0\t0\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t1\t1\n",
      "1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\n",
      "0\t0\t0\t1\t0\t0\t0\t0\t1\t0\t1\t0\t1\t1\n",
      "0\t1\t0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\n",
      "1\t1\t0\t0\t0\t1\t0\t1\t1\t1\t0\t1\t1\t1\n",
      "1\t1\t1\t0\t0\t0\t1\t1\t0\t0\t1\t1\t1\t0\n",
      "1\t1\t1\t1\t1\t0\t1\t0\t0\t0\t1\t0\t1\t0\n",
      "1\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t0\t1\t1\t1\t1\t0\t1\t0\t1\t1\t0\n",
      "0\t1\t1\t1\t0\t1\t1\t0\t0\t1\t1\t1\t1\t0\n",
      "1\t1\t1\t1\t1\t1\t1\t0\t0\t0\t1\t0\t1\t0\n",
      "0\t1\t1\t0\t0\t0\t1\t1\t1\t0\t1\t0\t1\t1\n",
      "0\t1\t0\t1\t0\t0\t0\t1\t0\t0\t1\t0\t1\t0\n",
      "1\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\t0\t1\t1\n",
      "1\t1\t1\t1\t0\t1\t0\t0\t1\t0\t0\t0\t1\t0\n",
      "0\t0\t1\t1\t1\t1\t0\t0\t0\t0\t0\t1\t1\t1\n",
      "0\t1\t1\t1\t1\t0\t0\t1\t0\t0\t0\t1\t1\t0\n",
      "1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0\t1\t1\t0\n",
      "1\t1\t1\t0\t1\t0\t0\t1\t0\t0\t1\t1\t1\t0\n",
      "1\t0\t1\t0\t1\t1\t1\t1\t0\t0\t1\t1\t1\t0\n",
      "0\t1\t0\t1\t0\t1\t1\t0\t0\t1\t1\t0\t1\t0\n",
      "1\t0\t0\t1\t0\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t0\n",
      "1\t1\t1\t1\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
      "1\t0\t1\t1\t0\t1\t1\t1\t0\t1\t1\t0\t1\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the datasets\n",
    "!curl http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
    "!curl http://huang.eng.unt.edu/CSCE-5218/test.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A69DxPSc8vNs",
    "outputId": "5440e602-8ecd-44cf-d48d-2e8b00cdcc52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 11244  100 11244    0     0   305k      0 --:--:-- --:--:-- --:--:--  313k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  2844  100  2844    0     0  77073      0 --:--:-- --:--:-- --:--:-- 79000\n"
     ]
    }
   ],
   "source": [
    "# Take a peek at the datasets\n",
    "!curl --output train.dat http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
    "!curl --output test.dat http://huang.eng.unt.edu/CSCE-5218/test.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFXHLhnhwiBR"
   },
   "source": [
    "### Build the Perceptron Model\n",
    "\n",
    "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "cXAsP_lw3QwJ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "\n",
    "# Corpus reader, all columns but the last one are coordinates;\n",
    "#   the last column is the label\n",
    "def read_data(file_name):\n",
    "    f = open(file_name, 'r')\n",
    "    data = []\n",
    "    # Discard header line\n",
    "    f.readline()\n",
    "    for instance in f.readlines():\n",
    "        if not re.search('\\t', instance): continue\n",
    "        instance = list(map(int, instance.strip().split('\\t')))\n",
    "        # Add a dummy input so that w0 becomes the bias\n",
    "        instance = [-1] + instance\n",
    "        data += [instance]\n",
    "    return data\n",
    "\n",
    "\n",
    "def dot_product(array1, array2):\n",
    "    # dot product of array 1 and array 2\n",
    "    dot_product = 0\n",
    "    for i,j in zip(array1,array2):\n",
    "        dot_product = dot_product + (i * j)\n",
    "    return dot_product\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    # outpout of sigmoid function on x\n",
    "    sig = (1.0 / (1.0 + math.exp(-1.0*x)))\n",
    "    return sig\n",
    "\n",
    "# The output of the model, which for the perceptron is \n",
    "# the sigmoid function applied to the dot product of \n",
    "# the instance and the weights\n",
    "def output(weight, instance):\n",
    "    # output of the model\n",
    "    output = sigmoid(dot_product(weight, instance))\n",
    "    return output\n",
    "\n",
    "# Predict the label of an instance; this is the definition of the perceptron\n",
    "# you should output 1 if the output is >= 0.5 else output 0\n",
    "def predict(weights, instance):\n",
    "    # prediction of the model\n",
    "    if output(weights, instance) >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Accuracy = percent of correct predictions\n",
    "def get_accuracy(weights, instances):\n",
    "    # You do not to write code like this, but get used to it\n",
    "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
    "                   for instance in instances])\n",
    "    return correct * 100 / len(instances)\n",
    "\n",
    "\n",
    "# Train a perceptron with instances and hyperparameters:\n",
    "#       lr (learning rate) \n",
    "#       epochs\n",
    "# The implementation comes from the definition of the perceptron\n",
    "#\n",
    "# Training consists on fitting the parameters which are the weights\n",
    "# that's the only thing training is responsible to fit\n",
    "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
    "#\n",
    "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
    "# We are updating weights in the opposite direction of the gradient of the error,\n",
    "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
    "def train_perceptron(instances, lr, epochs):\n",
    "\n",
    "    # weight with length of training data\n",
    "    weights = [0] * (len(instances[0])-1)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for instance in instances:\n",
    "            # loop each object in training data for an each epoch\n",
    "            in_value = dot_product(weights, instance)\n",
    "            output = sigmoid(in_value)\n",
    "            error = instance[-1] - output\n",
    "            \n",
    "            #output = predict(weights, instance)\n",
    "            #error = instance[-1] - output\n",
    "\n",
    "            # loop each weight and update it for row in epoch\n",
    "            for i in range(0, len(weights)):\n",
    "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adBZuMlAwiBT"
   },
   "source": [
    "## Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "50YvUza-BYQF",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19467441594729673, -0.04556779025352943, -0.07298282989725792, -0.05672949341520093, -0.06044937376069533, -0.11712938497604504, -0.10269362159600408, -0.17997715498664454, -0.06636358047339574, -0.014607431359386623, -0.022979728863794166, 0.05902086549351479, -0.14934958215587596, -0.19467441594729673]\n",
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "lr = 0.005\n",
    "epochs = 5\n",
    "weights = train_perceptron(instances_tr, lr, epochs)\n",
    "accuracy = get_accuracy(weights, instances_te)\n",
    "print(weights)\n",
    "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \" \n",
    "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBXkvaiQMohX"
   },
   "source": [
    "## Questions\n",
    "\n",
    "Answer the following questions. Include your implementation and the output for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCQ6BEk1CBlr"
   },
   "source": [
    "\n",
    "\n",
    "### Question 1\n",
    "\n",
    "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
    "```\n",
    "in_value = dot_product(weights, instance)\n",
    "output = sigmoid(in_value)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "Why don't we have the following code snippet instead?\n",
    "```\n",
    "output = predict(weights, instance)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "#### TODO Add your answer here (text only)\n",
    "\n",
    "```\n",
    "Outputs of train_perceptron() following code snippets\n",
    "\n",
    "When we use dot_product(), sigmoid() & error() funtions we can get the exact weights (values are accurate)\n",
    "\n",
    "in_value = dot_product(weights, instance)\n",
    "output = sigmoid(in_value)\n",
    "error = instance[-1] - output\n",
    "\n",
    "weights:\n",
    "\n",
    "[0.19467441594729673, -0.04556779025352943, -0.07298282989725792, -0.05672949341520093, -0.06044937376069533, -0.11712938497604504, -0.10269362159600408, -0.17997715498664454, -0.06636358047339574, -0.014607431359386623, -0.022979728863794166, 0.05902086549351479, -0.14934958215587596, -0.19467441594729673]\n",
    "\n",
    "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "\n",
    "output = predict(weights, instance)\n",
    "error = instance[-1] - output\n",
    "\n",
    "The weights are not accurate with predict function it should return either 0 or 1.\n",
    "\n",
    "weights:\n",
    "\n",
    "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 32.0\n",
    "\n",
    "When we use the first code snippet to obtain precise numbers for the weights. where we are immediately returning either 0 or 1 in predict.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU3c3m6YL2rK"
   },
   "source": [
    "### Question 2\n",
    "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
    "\n",
    "```\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
    "lr = [0.005, 0.01, 0.05]              # learning rate\n",
    "```\n",
    "\n",
    "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
    "of your code.The output should look like the following:\n",
    "```\n",
    "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "[and so on for all the combinations]\n",
    "```\n",
    "You will get different results with different hyperparameters.\n",
    "\n",
    "#### TODO Add your answer here (code and output in the format above)\n",
    "\n",
    "```\n",
    "Output:\n",
    "\n",
    "#tr: 20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 67.0\n",
    "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
    "#tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
    "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 78.0\n",
    "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
    "#tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 73.0\n",
    "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 77.0\n",
    "#tr: 20, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 74.0\n",
    "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
    "#tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
    "#tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
    "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
    "#tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
    "#tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
    "#tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 77.0\n",
    "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
    "#tr: 20, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 64.0\n",
    "#tr: 40, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
    "#tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 67.0\n",
    "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 70.0\n",
    "#tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
    "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
    "#tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
    "#tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
    "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
    "#tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
    "#tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
    "#tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 79.0\n",
    "#tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
    "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
    "#tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
    "#tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
    "#tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
    "#tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
    "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
    " \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "G-VKJOUu2BTp",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 67.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 73.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 20, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 71.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 20, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 64.0\n",
      "#tr: 40, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 67.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 70.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 79.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 69.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 76.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
    "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
    "\n",
    "for lr in lr_array:\n",
    "     for tr_size in tr_percent:\n",
    "        for epochs in num_epochs:\n",
    "            size =  round(len(instances_tr)*tr_size/100)\n",
    "            pre_instances = instances_tr[0:size]\n",
    "            weights = train_perceptron(pre_instances, lr, epochs)\n",
    "            accuracy = get_accuracy(weights, instances_te)\n",
    "            print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFB9MtwML24O"
   },
   "source": [
    "### Question 3\n",
    "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
    "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
    "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
    "   ```\n",
    "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "```\n",
    "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
    "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
    "\n",
    "#### TODO: Add your answer here (code and text)\n",
    "\n",
    "```\n",
    "3.A\n",
    "\n",
    "To achieve the best accuracy with the test dataset, it is typically advised to use the entire training dataset. This is due to the fact that a model can learn a more representative and robust set of characteristics with more training data, which can generalize to new data more effectively.\n",
    "\n",
    "However, in some circumstances, employing a smaller portion of the training dataset may result in quicker training times, which can be helpful when there are constraints on computational time or resources. In these circumstances, the model might still succeed in achieving a respectably high accuracy on the test dataset, but it will rely on the particular dataset and the model's complexity.\n",
    "\n",
    "The appropriate amount of training data may vary based on the model and dataset, but in general, the amount of data used for training is a hyperparameter that may be tweaked to get the greatest model performance. A balance must be struck between the volume of data used for training and the computational capabilities of the model. Additionally, the model's performance on a validation set must be assessed to make sure it is not overfitting the training data.\n",
    "\n",
    "3.B\n",
    "\n",
    "The second run used more training data, but this doesn't always suggest that the quality of the additional data was higher. The performance of the model could be harmed if the additional data is noisy or has inaccurate labels.\n",
    "\n",
    "Model overfitting: It's likely that the model has been overfit to the new information, which can have a detrimental effect on how well it performs on the test dataset. Overfitting happens when the model loses its ability to generalize to new data because it becomes too focused on the training set.\n",
    "\n",
    "Hyperparameter tuning: The second run's performance may have been impacted by the usage of different hyperparameters from the first. The model may have performed poorly despite the additional data if the hyperparameters were not chosen appropriately.\n",
    "\n",
    "Randomness: In some circumstances, random variation can have an impact on how well a machine learning model performs. For instance, the model's starting weights might have been initialized differently between the first and second runs, which might have affected the performance of the model.\n",
    "\n",
    "A more proper investigation of the data and model would be required to pinpoint the precise reason for the performance discrepancy between the first and second runs. This could entail performing additional trials with various hyperparameters or data subsets, comparing the learnt weights of the model, and assessing the training and test error curves.\n",
    "\n",
    "3.C\n",
    "\n",
    "By adding more hyperparameters, we cannot get accuracy higher than 80.0.\n",
    "\n",
    "Optimizing hyperparameters can be helpful in situations where the model is overfitting (performing effectively on a training set but poorly on the test dataset) or underfitting (performing poorly on the training set but effectively on the test dataset).\n",
    "\n",
    "Additional hyperparameters may be utilized to increase accuracy, however this will depend on the particular model and dataset being used. In general, adding additional hyperparameters or making the model more complicated might boost performance up to a point, but further complexity increases may result in overfitting and a decline in performance.\n",
    "\n",
    "3.D\n",
    "\n",
    "No, even when all other hyperparameters are fixed, it is not always worthwhile to train for longer epochs. In reality, overfitting, when the model starts to memorize the training data rather than learning generalizable patterns, can result from training a model for too many epochs. Poor performance on the test data and a decreased capacity to generalize to new data might result from overfitting.\n",
    "\n",
    "The best number of training epochs for a model varies on the dataset, model architecture, and difficulty of the problem being addressed. Generally speaking, it's critical to keep an eye on the training and validation loss during training to detect when the model has converged, and further iterations are unlikely to improve performance.\n",
    "\n",
    "In general, it's crucial to strike a balance between continuing training for long enough to let the model converge and ceasing it just in time to avoid overfitting. To determine the ideal number of epochs for a particular task, experimentation and observation of performance indicators like accuracy and loss are essential.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38rA_Kp3wiBX"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW2_The_Perceptron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
